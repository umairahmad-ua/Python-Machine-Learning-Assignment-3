{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Assignment No 3a\n",
    "### *Written By: Sibt ul Hussain*\n",
    "----\n",
    "## Goal\n",
    "\n",
    "Your goal in this part of assigment is to test your KNN Classifier on a kaggle competition. \n",
    "\n",
    "- The first competition is [Otto Group Product Classification Challenge](http://www.kaggle.com/c/otto-group-product-classification-challenge), here the goal is to place the given object in its right category (prize for winning this competition is US $10,000).\n",
    "\n",
    "For the fun part your can also test your code for the [digit classification](http://www.kaggle.com/c/digit-recognizer).\n",
    "\n",
    "\n",
    "**Note** Please note that you are allowed to use only those libraries which we have discussed in the class, i.e. numpy, scipy, pandas.\n",
    "\n",
    "## Submission Instructions\n",
    "You are required to submit the original notebook file on the Slate (with .ipynb extension), with complete set of outputs. Students failing to do so will get zero marks. \n",
    "\n",
    "*Please read each step carefully and understand it fully before proceeding with code writing*\n",
    "\n",
    "## Plagiarism\n",
    "Any form of plagiarism will not be tolerated and result in 0 marks.\n",
    "\n",
    "#### HelpFul Functions\n",
    "You might find following functions to be extremely helpful\n",
    " - **[argpartition] (http://docs.scipy.org/doc/numpy/reference/generated/numpy.argpartition.html)** uses introselect algorithm to perform an indirect partition along the given axis. This can lead to O(n) complexity instead of O(nlogn). Remember indexing start from 0.\n",
    " - **[itemfreq]()** a function that returns a 2-D array of item frequencies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import scipy.stats\n",
    "from collections import defaultdict\n",
    "plt.style.use('ggplot')\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: You have to implement the following class\n",
    "## Your code goes here...\n",
    "\n",
    "class KNearestNeighbor:\n",
    "    ''' Implements the KNearest Neigbours For Classification... '''\n",
    "    def __init__(self, k, scalefeatures=False):        \n",
    "        self.k=k\n",
    "        self.scalefeatures=scalefeatures\n",
    "        \n",
    "        pass    \n",
    "    \n",
    "    def compute_distances_two_loops(self, X):\n",
    "        \"\"\"\n",
    "        Compute the distance between each test point in X and each training point\n",
    "        in self.X_train using a nested loop over both the training data and the \n",
    "        test data.\n",
    "\n",
    "        Input:\n",
    "        X - An num_test x dimension array where each row is a test point.\n",
    "\n",
    "        Output:\n",
    "        dists - A num_test x num_train array where dists[i, j] is the distance\n",
    "                between the ith test point and the jth training point.\n",
    "        \"\"\"\n",
    "        num_test = X.shape[0]\n",
    "        num_train = self.X_train.shape[0]\n",
    "        dists = np.zeros((num_test, num_train))\n",
    "        for i in xrange(num_test):\n",
    "            for j in xrange(num_train):\n",
    "                #####################################################################\n",
    "                # TODO:                                                             #\n",
    "                # Compute the l2 distance between the ith test point and the jth    #\n",
    "                # training point, and store the result in dists[i, j]               #\n",
    "                #####################################################################\n",
    "                \n",
    "                #####################################################################\n",
    "                #                       END OF YOUR CODE                            #\n",
    "                #####################################################################\n",
    "        return dists\n",
    "\n",
    "    def compute_distances_one_loop(self, X):\n",
    "        \"\"\"\n",
    "        Compute the distance between each test point in X and each training point\n",
    "        in self.X_train using a single loop over the test data.\n",
    "\n",
    "        Input / Output: Same as compute_distances_two_loops\n",
    "        \"\"\"\n",
    "        num_test = X.shape[0]\n",
    "        num_train = self.X_train.shape[0]\n",
    "        dists = np.zeros((num_test, num_train))\n",
    "        for i in xrange(num_test):\n",
    "            #######################################################################\n",
    "            # TODO:                                                               #\n",
    "            # Compute the l2 distance between the ith test point and all training #\n",
    "            # points, and store the result in dists[i, :].                        #\n",
    "            #######################################################################\n",
    "            \n",
    "            \n",
    "            #######################################################################\n",
    "            #                         END OF YOUR CODE                            #\n",
    "            #######################################################################\n",
    "        return dists\n",
    "\n",
    "    def compute_distances_no_loops(self, X):\n",
    "        \"\"\"\n",
    "        Compute the distance between each test point in X and each training point\n",
    "        in self.X_train using no explicit loops.\n",
    "\n",
    "        Input / Output: Same as compute_distances_two_loops\n",
    "        \"\"\"\n",
    "        num_test = X.shape[0]\n",
    "        num_train = self.X_train.shape[0]\n",
    "        dists = np.zeros((num_test, num_train)) \n",
    "        #########################################################################\n",
    "        # TODO:                                                                 #\n",
    "        # Compute the l2 distance between all test points and all training      #\n",
    "        # points without using any explicit loops, and store the result in      #\n",
    "        # dists.                                                                #\n",
    "        # HINT: Try to formulate the l2 distance using matrix multiplication    #\n",
    "        #       and two broadcast sums.   \n",
    "        # Look at the np.repeat function as well                                #\n",
    "        #########################################################################\n",
    "        \n",
    "        \n",
    "        #########################################################################\n",
    "        #                         END OF YOUR CODE                              #\n",
    "        #########################################################################\n",
    "        return dists\n",
    "    def scale_features(self,X):\n",
    "        \"\"\"\n",
    "            Normalize each feature to lie in the range [0 ,1]\n",
    "\n",
    "            Input:\n",
    "            ------\n",
    "\n",
    "                X= M x d dimensional data matrix\n",
    "\n",
    "            Returns:\n",
    "            --------\n",
    "\n",
    "                normalized X\n",
    "        \"\"\"\n",
    "        # we will store these values compute on training set to use during testing \n",
    "        self.xmin= np.min(X,axis=0)\n",
    "        self.xmax= np.max(X,axis=0)\n",
    "\n",
    "        return (X-self.xmin)/(self.xmax-self.xmin)\n",
    "    def train(self, X, Y):\n",
    "        ''' Train K Nearest Neighbour classifier using the given \n",
    "            X [m x d] data matrix and Y labels matrix\n",
    "            \n",
    "            Input:\n",
    "            ------\n",
    "            X: [m x d] a data matrix of m d-dimensional examples.\n",
    "            Y: [m x 1] a label vector.\n",
    "            \n",
    "            Returns:\n",
    "            -----------\n",
    "            Nothing\n",
    "            '''\n",
    "        \n",
    "        \n",
    "        nexamples,nfeatures=X.shape\n",
    "        \n",
    "        if self.scalefeatures:\n",
    "            X=self.scale_features(X)\n",
    "        \n",
    "        \n",
    "        #Your code goes here...\n",
    "        #define self.X_train to store the training data...\n",
    "        \n",
    "        \n",
    "    \n",
    "    def predict(self, X, methodtype='noloops'):\n",
    "        \n",
    "        \"\"\"\n",
    "        Test the trained K-Nearset Neighoubr classifier result on the given examples X\n",
    "        \n",
    "                   \n",
    "            Input:\n",
    "            ------\n",
    "            X: [m x d] a matrix of m  d-dimensional test examples.\n",
    "            methodtype: which method to use for calculating distances.\n",
    "               noloops: without using any loop\n",
    "               oneloop: using one loop\n",
    "               twoloops: using two nested loops...\n",
    "               \n",
    "            Returns:\n",
    "            -----------\n",
    "                pclass: the predicted class for the given set of examples, i.e. to which it belongs\n",
    "        \"\"\"\n",
    "        \n",
    "        num_test = X.shape[0]\n",
    "        \n",
    "        if self.scalefeatures:\n",
    "            X=(X-self.xmin)/(self.xmax-self.xmin)\n",
    "        \n",
    "        y_pred = np.zeros(num_test, dtype = self.Y_train.dtype)\n",
    "        \n",
    "        # defining a function variable so that you will only need to call compute_distance\n",
    "        # for computing the distances...\n",
    "        if methodtype == 'noloops':\n",
    "            compute_distance = self.compute_distances_no_loops\n",
    "        elif methodtype == 'oneloop':\n",
    "            compute_distance = self.compute_distances_one_loop\n",
    "        else:\n",
    "            compute_distance = self.compute_distances_two_loops\n",
    "        #your code goes here...\n",
    "        \n",
    "        \n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert predicted classes to a matrix...\n",
    "def class_2_matrix(Y, nclasses):\n",
    "    '''\n",
    "    converts a vector of length m to a matrix of m x nclasses (required for computing log-loss and kaggle format),\n",
    "    Inputs:\n",
    "    ----------\n",
    "    \n",
    "        Y: a vector of length m \n",
    "        nclasses: number of classes in the dataset\n",
    "    \n",
    "    Retuns:\n",
    "    ----------\n",
    "        a binary matrix of size m X nclasses, with all columns of i_th example\n",
    "        set to zero except the one to which the class belongs (or predicted to belong).\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    ntest=len(Y)\n",
    "    pclassm=np.zeros((ntest,nclasses),np.uint8)\n",
    "\n",
    "    for i in range(len(Y)):\n",
    "        j=int(Y[i].split('_')[1])-1\n",
    "        pclassm[i,j]=1\n",
    "    \n",
    "    return pclassm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logloss(plabels,olabels):\n",
    "    '''\n",
    "        computes the logloss from the predicted labels and original labels vectors...\n",
    "        \n",
    "\n",
    "            logloss=−1N∑i=1N∑j=1Myi,jlog(pi,j)\n",
    "            \n",
    "        Input:\n",
    "        ---------\n",
    "        \n",
    "            plabels: predicted labels\n",
    "            olabels: original lables\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "            logloss: logistic loss for the given set of examples...\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    nclasses=len(np.unique(olabels))\n",
    "    \n",
    "    olabels=class_2_matrix(olabels,nclasses)\n",
    "    plabels=class_2_matrix(plabels,nclasses)\n",
    "    \n",
    "    epsilon = 1e-15\n",
    "    import scipy as sp\n",
    "    plabels = sp.maximum(epsilon, plabels) # clip the plabels\n",
    "    plabels = sp.minimum(1-epsilon, plabels)\n",
    "    logloss=-1.0/len(Ytest)*np.sum(np.log(plabels)*olabels)\n",
    "    \n",
    "    return logloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tools as t # set of tools for plotting, data splitting, etc.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otto Group Product Classification Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Download the dataset and use it\n",
    "#load the data set\n",
    "data=pd.read_csv('./train.csv')\n",
    "print data.describe()\n",
    "print data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y=data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seeing is beleaving, so lets start...\n",
    "#Lets first plot the frequency of each class...\n",
    "freq=scipy.stats.itemfreq(Y)# Get frequency of each class....\n",
    "plt.bar(np.arange(1,len(freq[:,1])+1),freq[:,1],width=0.35)\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Frequency of different class examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem with Accuracy as evaluation Metric\n",
    "Accuracy as evaluation metric works well when we have same number of examples for different classes, but for the datasets with imbalanced number of examples for each class accuracy is not preferred. For example consider you have dataset with 100 examples (95 from class-1 and 5 from class-2). Now having accuracy of 95% can mean correct classification of all the examples of class-1 with all examples of class-2 being miss-classified, which is not acceptable. So remember to never use the accuracy as evaluation metric for dataset with different number of examples for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get your data in matrix\n",
    "X=np.asarray(data.ix[:,1:-1].dropna(),dtype=np.float32)\n",
    "print X.shape\n",
    "Y=np.asarray(data.ix[:,-1])\n",
    "print Y, Y.shape, len(np.unique(Y)) # so we have 9 classes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Split your data into training and test-set... \n",
    "# see the documentation of split_data in tools for further information...\n",
    "Xtrain,Ytrain,Xtest,Ytest=t.split_data(X,Y)\n",
    "\n",
    "print \" Training Data Set Dimensions=\", Xtrain.shape, \"Training True Class labels dimensions\", Ytrain.shape   \n",
    "print \" Test Data Set Dimensions=\", Xtest.shape, \"Test True Class labels dimensions\", Ytest.shape   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Lets train a Decision Tree Classifier on Petal Length and Width\n",
    "feat=[0,1]\n",
    "knn=KNearestNeighbor(3) # train a 3-nearest neighbour classifier...\n",
    "knn.train(Xtrain[:,feat],Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_in_kaggle_format(filename,ids,pclasses,nclasses=9):\n",
    "    '''\n",
    "        write the result in kaggle required format...\n",
    "        \n",
    "        Input:\n",
    "        ----------\n",
    "            filename: name of file\n",
    "            ids: an array of ids of the examples\n",
    "            pclasses: predicted classes of each example...\n",
    "            \n",
    "        \n",
    "        Returns:\n",
    "        ----------\n",
    "            Nothing.\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    #write the result in the kaggle's required format\n",
    "    \n",
    "    pclasses=class_2_matrix(pclasses,nclasses)\n",
    "    \n",
    "    dic=defaultdict(list) #build dictionary...\n",
    "    dic['id']=ids\n",
    "    for i in range(pclasses.shape[1]):\n",
    "        dic['Class_'+str(i+1)]=pclasses[:,i]\n",
    "\n",
    "    output = pd.DataFrame(dic,columns=['id','Class_1','Class_2', 'Class_3', 'Class_4', 'Class_5', 'Class_6', 'Class_7', 'Class_8', 'Class_9'])\n",
    "\n",
    "    # Use pandas to write the comma-separated output file\n",
    "    output.to_csv( filename, index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Lets see how good we are doing...\n",
    "=================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Lets see how good we are doing, by finding the accuracy on the test set..\n",
    "#%debug\n",
    "\n",
    "def test_by_5000(Xtest,knn, subsetsize=5000):\n",
    "#lets split the test set into subsets of 5000 examples, to use our implementation without loops\n",
    "    print Xtest.shape\n",
    "\n",
    "    nsubsets=Xtest.shape[0]/subsetsize\n",
    "    remaining=Xtest.shape[0]%subsetsize\n",
    "    XtestSplit=np.split(Xtest[:nsubsets*subsetsize,:],nsubsets)\n",
    "    XtestSplit.append(Xtest[-remaining:,:])\n",
    "\n",
    "    pclasses=[]\n",
    "    for test in XtestSplit:\n",
    "        t=knn.predict(test)\n",
    "        pclasses.extend(list(t))\n",
    "    \n",
    "    return pclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%debug\n",
    "print logloss(np.array(pclasses),Ytest) # smaller this value is better your results are it is logloss...\n",
    "print len(pclasses),Xtest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling (or Normalization)\n",
    "\n",
    "Since we are using Euclidean distance to find the nearest neighbours, which is (as we have seen in the lectures) is heavily influenced by differently scaled features (that features having different scales and ranges). So to make best of K Nearest Neigbhour classifier we will be needed to first scale each feature dimension. Now lets go and write code for the feature scaling in KNearestNeighbour..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets train a KNN Classifier on Normalized Petal Length and Width\n",
    "feat=[0,1]\n",
    "knn=KNearestNeighbor(3,scalefeatures=True) # train a 3-nearest neighbour classifier...\n",
    "\n",
    "knn.train(Xtrain[:,feat],Ytrain)\n",
    "#Lets test it on the set of unseen examples...\n",
    "pclasses=test_by_5000(Xtest[:,feat],knn)#knn.predict(Xtest[:,feat])\n",
    "\n",
    "print logloss(pclasses,Ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Comment on the effect of feature scaling and normalization. \n",
    "\n",
    "What is the difference between the results of normalized features and non-normalized features. Why we are seeing the improvement (or decrease) in the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Lets Train on all features (can really bake your machine)...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Lets train a KNN on all the features... will take around 1 hour\n",
    "feat=np.arange(Xtrain.shape[1])\n",
    "knn=KNearestNeighbor(100)# With K as a random guess...\n",
    "knn.train(Xtrain[:,feat],Ytrain)\n",
    "pclasses=test_by_5000(Xtest[:,feat],knn)\n",
    "#Lets see how good we are doing, by finding the logloss on the test set..\n",
    "print \"LogLoss = \", logloss(pclasses,Ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on original training and kaggle test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datatest=pd.read_csv('./data/otto-group-classification/test.csv')\n",
    "ids=np.array(datatest['id'])\n",
    "Xtest=np.array(datatest.ix[:,1:].values,dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets train on 10 randomly selected features \n",
    "feat=np.arange(X.shape[1])\n",
    "np.random.shuffle(feat)#using only ten features X.shape[1])\n",
    "feat=feat[:10]\n",
    "print feat\n",
    "knn=KNearestNeighbor(100)# With K as a random guess...\n",
    "knn.train(X[:,feat],Y)\n",
    "pclasses=knn.predict(Xtest[:,feat])\n",
    "#Lets see how good we are doing, by finding the logloss on the test set..\n",
    "print \"LogLoss = \", logloss(pclasses,Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Write in Kaggle Format...\n",
    "write_in_kaggle_format('kaggel-otto-5features-100-nn',ids,pclasses)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your rank should be around 2500, still better than many with using a simple classifier and random settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Lets train a KNN on all the features...\n",
    "\n",
    "#Becareful it can take around 3 Hours on an i5 machine to complete this, you can use time.time() function to time your code.\n",
    "\n",
    "\n",
    "feat=np.arange(X.shape[1])\n",
    "knn=KNearestNeighbor(100)# With K as a random guess...\n",
    "knn.train(X[:,feat],Y)\n",
    "pclasses=test_by_5000(Xtest[:,feat],knn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_in_kaggle_format('kaggel-otto-Allfeatures-100-nn',ids,pclasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What can you conclude ?\n",
    "====================\n",
    "Please write your observation....\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Cross-Validation [For Improving your Rank]\n",
    "\n",
    "Until now we have been splitting the dataset into a training and test set rather randomly and were reporting a rather artifical performance. Now we are going to test our system exhaustively by making use of k-fold [cross validation](http://en.wikipedia.org/wiki/Cross-validation_%28statistics%29). \n",
    "\n",
    "Now go and tune your hyper-parameters (K in this case) to opitmize the performance for only first two parameters.\n",
    "\n",
    "\n",
    "However here we will have to use a trick for better performance. We will train only a single classifier per fold with the maximum K value in the grid. Then we will simply decrease the value of K each time and see its impact. \n",
    "\n",
    "** Still you will require around 12 Hours to find the best paramaters, so be careful. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now lets cross validate for best paramters, and test the result...\n",
    "# We will be training four different models on four different partitions of data set and \n",
    "# then will be reporting the mean logloss of the four classifiers.\n",
    "\n",
    "nfolds=4 # lets use four folds..\n",
    "folds=t.generate_folds(X,Y,nfolds)\n",
    "features=[0,1] # features to use for our system\n",
    "#now lets train and test on these folds...\n",
    "\n",
    "#Lets perform the grid search...\n",
    "ks=np.arange(1,200,10) # search the k in the range one to 20...\n",
    "foldacc=[]          \n",
    "# Your code goes here...\n",
    "\n",
    "\n",
    "\n",
    "bestk=np.argmin(foldacc)    \n",
    "print '\\n\\n Best value for the K={} and gives following mean logloss={}'.format(ks[bestk],foldacc[bestk])\n",
    "bestk=ks[bestk]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrain the classifier with best-k and see its accuracy on kaggle\n",
    "### Add the screenshow below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
